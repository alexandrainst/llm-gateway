services:
  redis:
    image: redis:8.2
    volumes:
      - redis-data:/data
    command: ["redis-server", "--appendonly", "yes"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 5s

  orchestrator:
    build: ./orchestrator
    container_name: orchestrator
    env_file:
      - .env
    environment:
      # Auth keys file path inside the orchestrator container
      AUTH_KEYS_FILE: ${AUTH_KEYS_FILE:-/app-config/auth.keys.json}
      # HuggingFace token for downloading gated models
      HF_TOKEN: ${HF_TOKEN}
      # Note: HOST_HF_CACHE is not set here - the code uses /host_hf_cache by default
      # Runtime type: "sglang" or "vllm"
      RUNTIME_TYPE: ${RUNTIME_TYPE:-sglang}
      # Runtime image to launch (override as needed)
      # For sglang: lmsysorg/sglang:b200-cu129
      # For vLLM: vllm/vllm-openai:latest
      RUNTIME_IMAGE: ${RUNTIME_IMAGE:-lmsysorg/sglang:b200-cu129}
      # Extra args for the runtime
      # sglang example: --tp 2 --cuda-graph-max-bs 16
      # vLLM example: --tensor-parallel-size 2 --max-model-len 8192
      RUNTIME_ARGS: ${RUNTIME_ARGS:---tp 2 --cuda-graph-max-bs 16}
      RUNTIME_PORT: ${RUNTIME_PORT:-30000}
      REALTIME_MODEL: ${REALTIME_MODEL:-synquid/gemma-3-27b-it-FP8}
      MIN_HOT_TTL_S: ${MIN_HOT_TTL_S:-600}
      L_MAX_S: ${L_MAX_S:-600}
      P_MAX: ${P_MAX:-0.3}
      TOKENS_B: ${TOKENS_B:-2}
      TOKENS_REFILL_PER_SEC: ${TOKENS_REFILL_PER_SEC:-0.000555}
      # Disable progress bars for huggingface_hub downloads
      HF_HUB_DISABLE_PROGRESS_BARS: "1"
      TQDM_DISABLE: "1"
      PYTHONDONTWRITEBYTECODE: "1"
      PYTHONUNBUFFERED: "1"
    ports:
      - "8000:8000"  # Adjust port as needed for your orchestrator
    # bind the docker socket so orchestrator can start/stop runtime containers
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      # Mount auth keys file into the container at /app-config/auth.keys.json
      - ./config/auth.keys.json:/app-config/auth.keys.json:ro
      - ${HOST_HF_CACHE:-/home/alex-admin/.cache/huggingface}:/host_hf_cache:rw
      - ./orchestrator:/app:ro  # Mount entire app read-only for dev
      - ./eval-repo:/home/alex-admin/llm-gateway/eval-repo:rw  # Mount eval repo for running evals (rw for venv creation)
      - ./EuroEval:/home/alex-admin/llm-gateway/EuroEval:ro  # Mount modified EuroEval package
      - ./data:/data:rw  # Mount data directory for eval results
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request,sys; sys.exit(0) if urllib.request.urlopen(\"http://localhost:8000/healthz\").status==200 else sys.exit(1)' "]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 10s
    # Add --reload flag for auto-reload on code changes
    command: ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000", "--log-level", "info", "--reload"]

volumes:
  redis-data:
