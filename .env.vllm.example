# ==========================
# Orchestrator Environment
# (Copy to .env and adjust)
# ==========================

# Orchestrator auth: static scoped keys JSON (required)
# Compose mounts ./config/auth.keys.json to /app-config/auth.keys.json in the container
AUTH_KEYS_FILE=/app-config/auth.keys.json

# Redis connection
REDIS_URL=redis://redis:6379

# Runtime: vLLM (recommended)
RUNTIME_TYPE=vllm
RUNTIME_IMAGE=vllm/vllm-openai:latest
# Adjust tensor-parallel to your GPU count
RUNTIME_ARGS=--tensor-parallel-size 2
RUNTIME_PORT=30000

# Realtime model kept hot (example)
# Replace with a model available to your setup
REALTIME_MODEL=synquid/gemma-3-27b-it-FP8

# Host Hugging Face cache mount inside orchestrator container
# Ensure docker-compose mounts your host cache to this path
HOST_HF_CACHE=/host_hf_cache

# Optional: specify the host path for vLLM compile cache
# Set to an absolute host path to persist torch.compile artifacts
# VLLM_CACHE_HOST=/home/you/.cache/vllm

# Optional: Hugging Face token for gated models/tokenizer configs
# HF_TOKEN=

# Realtime protection (defaults are reasonable)
# MIN_HOT_TTL_S=600
# TOKENS_B=3
# TOKENS_REFILL_PER_SEC=0.000555
# REALTIME_COOLDOWN_S=120

# Startup/readiness timeouts
# DEFAULT_READY_TIMEOUT_S=120
# VLLM_READY_TIMEOUT_S=900

# Batch processing
# MAX_CONCURRENT_BATCH=64

# UI/Examples convenience (optional)
BASE_URL=http://localhost:8000

# Eval system
# EVAL_REPO_PATH=./eval-repo
