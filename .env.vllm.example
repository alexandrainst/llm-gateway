# Example configuration for vLLM runtime

# API authentication
API_KEY=changeme

# HuggingFace token for gated models
HF_TOKEN=your_hf_token_here

# Runtime configuration for vLLM
RUNTIME_TYPE=vllm
RUNTIME_IMAGE=vllm/vllm-openai:latest
RUNTIME_ARGS=--tensor-parallel-size 2 --max-model-len 8192 --gpu-memory-utilization 0.95

# Model to load on startup
REALTIME_MODEL=meta-llama/Llama-3.1-8B-Instruct

# Runtime port (vLLM serves OpenAI API on this port)
RUNTIME_PORT=30000

# Cache settings
HOST_HF_CACHE=/home/alex-admin/.cache/huggingface

# Rate limiting
TOKENS_B=2
TOKENS_REFILL_PER_SEC=0.000555

# Batch scheduling
MIN_HOT_TTL_S=600
L_MAX_S=600
P_MAX=0.3